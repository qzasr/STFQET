time_slot=15, torch_seed=46, num_link=223, num_his=8, num_pred=8, L=2, K=8, d=12, train_ratio=0.7, val_ratio=0.1, test_ratio=0.2, batch_size=8, max_epoch=100, patience=8, learning_rate=0.003, decay_epoch=20, traffic_file='./data/Traffic_speed_(24, 33).csv', query_file='./data/query_beijing_0201_grid_start_destination_count.npz', SE_file='./data/Gf_grid_24_33_id.txt', adj_file='./data/edge_list_grid_(24,33)_weight.txt', row=24, col=33, model_file='./save_model/grid_24_33/Model_GF_test.pkl', log_file='./log/grid_24_33/log_test', STE_SE_FC_bn_decay=0.1, STE_SE_FC_drop=0.07, STE_SE_FC_act1='relu', STE_SE_FC_act2='none', STE_TE_FC_bn_decay=0.1, STE_TE_FC_drop=0.01, STE_TE_FC_act1='relu', STE_TE_FC_act2='none', FFT_FC_bn_decay=0.05, FFT_FC_drop=0.01, FFT_FC_act1='relu', FFT_FC_act2='none', KNA_W_bn_decay=0.1, KNA_W_drop=0.01, KNA_W_act1='relu', KNA_W_act2='none', KNA_Wq_bn_decay=0.1, KNA_Wq_drop=0.01, KNA_Wq_act='none', KNA_Wk_bn_decay=0.1, KNA_Wk_drop=0.01, KNA_Wk_act='none', KNA_Wv_bn_decay=0.1, KNA_Wv_drop=0.01, KNA_Wv_act='none', KNA_Fusion_bn_decay=0.1, KNA_Fusion_drop=0.01, KNA_Fusion_act='relu', SA_FCq_bn_decay=0.1, SA_FCq_drop=0.01, SA_FCq_act='relu', SA_FCk_bn_decay=0.1, SA_FCk_drop=0.01, SA_FCk_act='relu', SA_FCv_bn_decay=0.1, SA_FCv_drop=0.01, SA_FCv_act='relu', SA_FC_bn_decay=0.1, SA_FC_drop=0.01, SA_FC_act='relu', TA_Wq_bn_decay=0.1, TA_Wq_drop=0.01, TA_Wq_act='none', TA_Wk_bn_decay=0.1, TA_Wk_drop=0.01, TA_Wk_act='none', TA_Wv_bn_decay=0.1, TA_Wv_drop=0.01, TA_Wv_act='none', TA_Fusion_bn_decay=0.1, TA_Fusion_drop=0.01, TA_Fusion_act='relu', NAV_Wq_bn_decay=0.1, NAV_Wq_drop=0.01, NAV_Wq_act='none', NAV_Wk_bn_decay=0.1, NAV_Wk_drop=0.01, NAV_Wk_act='none', NAV_Wv_bn_decay=0.1, NAV_Wv_drop=0.01, NAV_Wv_act='none', NAV_Mapping_bn_decay=0.1, NAV_Mapping_drop=0.01, NAV_Mapping_act='relu', NAV_Fusion_bn_decay=0.1, NAV_Fusion_drop=0.01, NAV_Fusion_act='relu', STB_SpatialFusion_bn_decay=0.1, STB_SpatialFusion_drop=0.01, STB_SpatialFusion_act='relu', STB_TemporalFusion_bn_decay=0.1, STB_TemporalFusion_drop=0.01, STB_TemporalFusion_act='relu', STB_FinalFusion_bn_decay=0.1, STB_FinalFusion_drop=0.01, STB_FinalFusion_act='relu', STB_Gate_bn_decay=0.1, STB_Gate_drop=0.01, STB_Gate_act1='relu', STB_Gate_act2='sigmoid', TA_FCq_bn_decay=0.1, TA_FCq_drop=0.01, TA_FCq_act='relu', TA_FCk_bn_decay=0.1, TA_FCk_drop=0.01, TA_FCk_act='relu', TA_FCv_bn_decay=0.1, TA_FCv_drop=0.01, TA_FCv_act='relu', TA_FC_bn_decay=0.1, TA_FC_drop=0.01, TA_FC_act='relu', Traffic_Linear_bn_decay=0.1, Traffic_Linear_drop=0.01, Traffic_Linear_act1='relu', Traffic_Linear_act2='none', Query_Linear_bn_decay=0.1, Query_Linear_drop=0.01, Query_Linear_act1='relu', Query_Linear_act2='none', Output_bn_decay=0.1, Output_drop=0.01, Output_act1='relu', Output_act2='none', GDC_bn_decay=0.1, TSP_bn_decay=0.1
Using device: cuda:0
trainX: torch.Size([4084, 8, 223])		 trainY: torch.Size([4084, 8, 223])
valX:   torch.Size([571, 8, 223])		valY:   torch.Size([571, 8, 223])
testX:   torch.Size([1156, 8, 223])		testY:   torch.Size([1156, 8, 223])
mean:   31.8979		std:   10.7164
data loaded!
compiling model...
trainable parameters: 1,419,136
**** training model ****
2025-08-29 03:25:29 | epoch: 0001/100, training time: 94.0s, inference time: 2.6s
train loss: 2.7966, val_loss: 2.1960
val loss decrease from inf to 2.1960, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-29 03:27:02 | epoch: 0002/100, training time: 90.4s, inference time: 2.5s
train loss: 2.4888, val_loss: 2.1234
val loss decrease from 2.1960 to 2.1234, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-29 03:28:38 | epoch: 0003/100, training time: 93.1s, inference time: 2.6s
train loss: 2.4294, val_loss: 2.0963
val loss decrease from 2.1234 to 2.0963, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-29 03:30:11 | epoch: 0004/100, training time: 91.1s, inference time: 2.5s
train loss: 2.3964, val_loss: 2.0964
2025-08-29 03:31:42 | epoch: 0005/100, training time: 88.1s, inference time: 2.5s
train loss: 2.3450, val_loss: 2.1022
2025-08-29 03:33:13 | epoch: 0006/100, training time: 88.4s, inference time: 2.5s
train loss: 2.3311, val_loss: 2.0660
val loss decrease from 2.0963 to 2.0660, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-29 03:34:43 | epoch: 0007/100, training time: 88.2s, inference time: 2.5s
train loss: 2.3253, val_loss: 2.0791
2025-08-29 03:36:15 | epoch: 0008/100, training time: 89.0s, inference time: 2.5s
train loss: 2.2998, val_loss: 2.0632
val loss decrease from 2.0660 to 2.0632, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-29 03:37:44 | epoch: 0009/100, training time: 87.2s, inference time: 2.5s
train loss: 2.2760, val_loss: 2.0653
2025-08-29 03:39:16 | epoch: 0010/100, training time: 89.0s, inference time: 2.5s
train loss: 2.3169, val_loss: 2.0406
val loss decrease from 2.0632 to 2.0406, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-29 03:40:47 | epoch: 0011/100, training time: 88.3s, inference time: 2.5s
train loss: 2.2645, val_loss: 2.0597
2025-08-29 03:42:17 | epoch: 0012/100, training time: 87.9s, inference time: 2.5s
train loss: 2.2962, val_loss: 2.0538
2025-08-29 03:43:51 | epoch: 0013/100, training time: 91.5s, inference time: 2.5s
train loss: 2.2461, val_loss: 2.0494
2025-08-29 03:45:23 | epoch: 0014/100, training time: 89.2s, inference time: 2.5s
train loss: 2.2653, val_loss: 2.0601
2025-08-29 03:46:53 | epoch: 0015/100, training time: 87.5s, inference time: 2.5s
train loss: 2.2663, val_loss: 2.0900
2025-08-29 03:48:24 | epoch: 0016/100, training time: 89.0s, inference time: 2.5s
train loss: 2.2124, val_loss: 2.0828
2025-08-29 03:49:54 | epoch: 0017/100, training time: 86.7s, inference time: 2.5s
train loss: 2.2577, val_loss: 2.0257
val loss decrease from 2.0406 to 2.0257, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-29 03:51:20 | epoch: 0018/100, training time: 84.4s, inference time: 2.5s
train loss: 2.2539, val_loss: 2.0549
2025-08-29 03:52:50 | epoch: 0019/100, training time: 86.9s, inference time: 2.5s
train loss: 2.2081, val_loss: 2.0448
2025-08-29 03:54:21 | epoch: 0020/100, training time: 88.4s, inference time: 2.5s
train loss: 2.2013, val_loss: 2.0333
2025-08-29 03:55:51 | epoch: 0021/100, training time: 87.8s, inference time: 2.5s
train loss: 2.1973, val_loss: 2.0228
val loss decrease from 2.0257 to 2.0228, saving model to ./save_model/grid_24_33/Model_GF_test.pkl
2025-08-29 03:57:25 | epoch: 0022/100, training time: 91.2s, inference time: 2.5s
train loss: 2.1965, val_loss: 2.0556
2025-08-29 03:58:56 | epoch: 0023/100, training time: 88.5s, inference time: 2.5s
train loss: 2.1529, val_loss: 2.0982
2025-08-29 04:00:26 | epoch: 0024/100, training time: 88.1s, inference time: 2.5s
train loss: 2.1783, val_loss: 2.0338
2025-08-29 04:01:57 | epoch: 0025/100, training time: 88.6s, inference time: 2.5s
train loss: 2.1854, val_loss: 2.0498
2025-08-29 04:03:31 | epoch: 0026/100, training time: 91.0s, inference time: 2.5s
train loss: 2.1576, val_loss: 2.0663
2025-08-29 04:05:01 | epoch: 0027/100, training time: 87.5s, inference time: 2.5s
train loss: 2.1412, val_loss: 2.0340
2025-08-29 04:06:31 | epoch: 0028/100, training time: 87.6s, inference time: 2.5s
train loss: 2.1780, val_loss: 2.0548
2025-08-29 04:08:01 | epoch: 0029/100, training time: 87.8s, inference time: 2.5s
train loss: 2.1810, val_loss: 2.0553
early stop at epoch: 0029
Training completed. Best model saved to ./save_model/grid_24_33/Model_GF_test.pkl
**** testing model ****
loading model from ./save_model/grid_24_33/Model_GF_test.pkl
model loaded!
evaluating...
train MAE: 1.7436, RMSE: 2.6806, MAPE: 5.91%
performance in each prediction step (train)
step 1: MAE=1.4974, RMSE=2.2440, MAPE=5.09%
step 2: MAE=1.7569, RMSE=2.7054, MAPE=5.95%
step 3: MAE=1.7670, RMSE=2.7288, MAPE=5.99%
step 4: MAE=1.7655, RMSE=2.7245, MAPE=5.98%
step 5: MAE=1.7644, RMSE=2.7226, MAPE=5.98%
step 6: MAE=1.7701, RMSE=2.7305, MAPE=6.00%
step 7: MAE=1.7957, RMSE=2.7678, MAPE=6.09%
step 8: MAE=1.8315, RMSE=2.8213, MAPE=6.22%
average: MAE=1.7436, RMSE=2.6806, MAPE=5.91%
val MAE: 2.0560, RMSE: 3.1542, MAPE: 7.01%
performance in each prediction step (val)
step 1: MAE=1.6706, RMSE=2.4646, MAPE=5.67%
step 2: MAE=2.0306, RMSE=3.0843, MAPE=6.92%
step 3: MAE=2.0869, RMSE=3.2110, MAPE=7.12%
step 4: MAE=2.1080, RMSE=3.2549, MAPE=7.19%
step 5: MAE=2.1253, RMSE=3.2870, MAPE=7.25%
step 6: MAE=2.1346, RMSE=3.3010, MAPE=7.29%
step 7: MAE=2.1427, RMSE=3.3122, MAPE=7.31%
step 8: MAE=2.1495, RMSE=3.3185, MAPE=7.33%
average: MAE=2.0560, RMSE=3.1542, MAPE=7.01%
test MAE: 2.1139, RMSE: 3.2341, MAPE: 7.22%
performance in each prediction step (test)
step 1: MAE=1.7030, RMSE=2.5117, MAPE=5.80%
step 2: MAE=2.0835, RMSE=3.1750, MAPE=7.11%
step 3: MAE=2.1462, RMSE=3.2958, MAPE=7.33%
step 4: MAE=2.1703, RMSE=3.3403, MAPE=7.41%
step 5: MAE=2.1859, RMSE=3.3682, MAPE=7.47%
step 6: MAE=2.1963, RMSE=3.3803, MAPE=7.51%
step 7: MAE=2.2064, RMSE=3.3921, MAPE=7.54%
step 8: MAE=2.2195, RMSE=3.4097, MAPE=7.59%
average: MAE=2.1139, RMSE=3.2341, MAPE=7.22%
total testing time: 25.3s
total time: 44.8min
